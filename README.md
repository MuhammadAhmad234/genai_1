# ğŸ¤– GenAI-1 â€” Learn Generative AI from Scratch

A **beginner-friendly** hands-on project to learn and grasp the fundamentals of **Generative AI** using [LangChain](https://www.langchain.com/) and popular LLM providers. Each folder is a self-contained learning module that progressively builds your understanding â€” from invoking your first LLM to building a fully functional chatbot.

---

## ğŸ“š What You'll Learn

| Module               | Key Concepts                                                                 |
| -------------------- | ---------------------------------------------------------------------------- |
| **LLMs**             | Invoking a large language model for the first time                           |
| **ChatModels**       | Using Chat Models with OpenAI, HuggingFace (cloud & local)                   |
| **EmbeddedModels**   | Text embeddings, document embeddings & semantic similarity                   |
| **Prompts**          | Prompt Templates, saving/loading templates, Streamlit UI                     |
| **Chatbot**          | LangChain messaging, ChatPromptTemplate, MessagesPlaceholder & CLI chatbot   |
| **StructuredOutput** | Structured LLM responses using TypedDict & Pydantic schemas                  |
| **OutputParsers**    | StrOutputParser, JsonOutputParser, StructuredOutputParser & chains           |
| **Chains**           | Simple, Sequential, Parallel & Conditional chains with RunnableBranch        |
| **Runnables**        | Why Runnables exist â€” the old-way problem & the unified `.invoke()` solution |

---

## ğŸ—‚ï¸ Project Structure

```
genai_1/
â”œâ”€â”€ LLMS/
â”‚   â””â”€â”€ llm_demo.py                  # Basic LLM invocation with GPT-3.5-Turbo
â”œâ”€â”€ ChatModels/
â”‚   â”œâ”€â”€ openaiChatDemo.py            # Chat with OpenAI GPT-4
â”‚   â”œâ”€â”€ huggingfaceChatDemo.py       # Chat using HuggingFace cloud endpoint
â”‚   â””â”€â”€ chatModelLocal.py           # Chat using a local HuggingFace model
â”œâ”€â”€ EmbeddedModels/
â”‚   â”œâ”€â”€ embeddingOpenai.py           # Generate embeddings for a single query
â”‚   â”œâ”€â”€ embeddingOpenaiDoc.py        # Generate embeddings for multiple documents
â”‚   â””â”€â”€ embeddingDocSimilarity.py    # Document similarity search with cosine similarity
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ promptGenerator.py           # Create & save a reusable PromptTemplate
â”‚   â””â”€â”€ promptUi.py                  # Streamlit UI â€” Research Paper Summarizer
â”œâ”€â”€ chatbot/
â”‚   â”œâ”€â”€ messages.py                  # LangChain built-in messaging (System, Human, AI)
â”‚   â”œâ”€â”€ chatbot.py                   # Interactive CLI chatbot with chat history
â”‚   â”œâ”€â”€ chatPromptTemplate.py        # Using ChatPromptTemplate with system & human messages
â”‚   â”œâ”€â”€ messagePlaceholder.py        # MessagesPlaceholder for injecting chat history
â”‚   â””â”€â”€ chatHistory.txt              # Sample chat history for MessagesPlaceholder demo
â”œâ”€â”€ structuredOutput/
â”‚   â”œâ”€â”€ typeDictDemo.py              # Basic TypedDict usage example
â”‚   â”œâ”€â”€ structuredOutputTypeDict.py  # Structured output with TypedDict schema
â”‚   â”œâ”€â”€ pydanticDemo.py              # Basic Pydantic BaseModel usage example
â”‚   â””â”€â”€ structuredOutputPydantic.py  # Structured output with Pydantic schema
â”œâ”€â”€ outputParsers/
â”‚   â”œâ”€â”€ strOutputParserDemo.py       # Chaining prompts without a parser
â”‚   â”œâ”€â”€ strOutputParser.py           # StrOutputParser with LangChain chains
â”‚   â”œâ”€â”€ jsonOutputParser.py          # JsonOutputParser for JSON responses
â”‚   â””â”€â”€ structureOutputParser.py     # StructuredOutputParser with ResponseSchema
â”œâ”€â”€ chains/
â”‚   â”œâ”€â”€ simpleChain.py               # Basic prompt | model | parser chain
â”‚   â”œâ”€â”€ sequentialChain.py           # Multi-step sequential chain (report â†’ summary)
â”‚   â”œâ”€â”€ parallelChain.py             # RunnableParallel â€” run branches concurrently & merge
â”‚   â””â”€â”€ conditionalChain.py          # RunnableBranch â€” route based on sentiment classification
â”œâ”€â”€ runnables/
â”‚   â”œâ”€â”€ simpleLLM.py                 # Old-way LLM usage (pre-Runnable, using .predict())
â”‚   â”œâ”€â”€ chainsProblem.ipynb          # The problem â€” inconsistent interfaces (.predict, .format, .run)
â”‚   â””â”€â”€ chainsSolution.ipynb         # The Runnable solution â€” unified .invoke() & runnableConnector
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ template.json                    # Saved prompt template (generated by promptGenerator.py)
â””â”€â”€ .env                             # API keys (not committed)
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.10+**
- API keys for one or more providers (see [API Keys](#-api-keys) below)

### Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/<your-username>/genai_1.git
   cd genai_1
   ```

2. **Create & activate a virtual environment**

   ```bash
   python -m venv venv

   # Windows
   venv\Scripts\activate

   # macOS / Linux
   source venv/bin/activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**

   Create a `.env` file in the project root and add your API keys:

   ```env
   OPENAI_API_KEY=your_openai_api_key
   HUGGINGFACEHUB_API_TOKEN=your_huggingface_api_token
   ```

---

## ğŸ“– Module Walkthroughs

### 1. LLMs â€” Your First LLM Call

> **File:** `LLMS/llm_demo.py`

The simplest starting point â€” invoke OpenAI's `gpt-3.5-turbo` with a single prompt and print the response.

```bash
python LLMS/llm_demo.py
```

---

### 2. ChatModels â€” Exploring Different Providers

> **Files:** `ChatModels/`

Learn how to use **Chat Models** from multiple providers:

| File                     | What It Does                                         |
| ------------------------ | ---------------------------------------------------- |
| `openaiChatDemo.py`      | Chat with **OpenAI GPT-4**                           |
| `huggingfaceChatDemo.py` | Chat via **HuggingFace Inference API** (cloud)       |
| `chatModelLocal.py`      | Chat using a **local HuggingFace model** (TinyLlama) |

```bash
python ChatModels/openaiChatDemo.py
python ChatModels/huggingfaceChatDemo.py
python ChatModels/chatModelLocal.py
```

---

### 3. EmbeddedModels â€” Text Embeddings & Similarity

> **Files:** `EmbeddedModels/`

Understand how text gets converted into numerical vectors and how to find the most relevant document for a given query.

| File                        | What It Does                                               |
| --------------------------- | ---------------------------------------------------------- |
| `embeddingOpenai.py`        | Generate an embedding vector for a **single query**        |
| `embeddingOpenaiDoc.py`     | Generate embeddings for **multiple documents**             |
| `embeddingDocSimilarity.py` | Find the **most similar document** using cosine similarity |

```bash
python EmbeddedModels/embeddingOpenai.py
python EmbeddedModels/embeddingDocSimilarity.py
```

---

### 4. Prompts â€” Templates & Streamlit UI

> **Files:** `prompts/`

Learn how to create structured, reusable **Prompt Templates** and build a simple web interface:

- **`promptGenerator.py`** â€” Define a prompt template with input variables and save it as `template.json`
- **`promptUi.py`** â€” A **Streamlit** web app that loads the saved template and lets you summarize research papers interactively

```bash
# Generate the template
python prompts/promptGenerator.py

# Launch the Streamlit UI
streamlit run prompts/promptUi.py
```

---

### 5. Chatbot â€” Messaging, Templates & CLI Chatbot

> **Files:** `chatbot/`

Build a conversational AI chatbot that remembers context, and learn how LangChain handles messages:

- **`messages.py`** â€” Understand LangChain's built-in message types (`SystemMessage`, `HumanMessage`, `AIMessage`)
- **`chatbot.py`** â€” A fully interactive **CLI chatbot** that maintains chat history across turns
- **`chatPromptTemplate.py`** â€” Use `ChatPromptTemplate` with system & human message placeholders
- **`messagePlaceholder.py`** â€” Use `MessagesPlaceholder` to inject prior chat history from a file
- **`chatHistory.txt`** â€” Sample conversation data used by the placeholder demo

```bash
# Understand messaging
python chatbot/messages.py

# ChatPromptTemplate demo
python chatbot/chatPromptTemplate.py

# MessagesPlaceholder demo
python chatbot/messagePlaceholder.py

# Run the chatbot (type "exit" to quit)
python chatbot/chatbot.py
```

---

### 6. Structured Output â€” TypedDict & Pydantic Schemas

> **Files:** `structuredOutput/`

Learn how to get **structured, predictable responses** from LLMs instead of free-form text. Define a schema and LangChain will ensure the model's output conforms to it.

- **`typeDictDemo.py`** â€” Understand Python's `TypedDict` for defining typed dictionaries
- **`structuredOutputTypeDict.py`** â€” Use `TypedDict` + `Annotated` to define a review schema and extract structured data from a product review
- **`pydanticDemo.py`** â€” Understand Pydantic's `BaseModel` for data validation
- **`structuredOutputPydantic.py`** â€” Use Pydantic `BaseModel` + `Field` to define a review schema with field descriptions

```bash
# TypedDict basics
python structuredOutput/typeDictDemo.py

# Structured output with TypedDict
python structuredOutput/structuredOutputTypeDict.py

# Pydantic basics
python structuredOutput/pydanticDemo.py

# Structured output with Pydantic
python structuredOutput/structuredOutputPydantic.py
```

---

### 7. Output Parsers â€” Parsing & Chaining LLM Responses

> **Files:** `outputParsers/`

Learn how to **parse and transform** LLM responses using LangChain's output parsers, and how to **chain** multiple prompts together:

- **`strOutputParserDemo.py`** â€” Chain two prompts sequentially (generate text â†’ summarize) without using a parser
- **`strOutputParser.py`** â€” Use `StrOutputParser` to build a clean LangChain chain (`template | model | parser`)
- **`jsonOutputParser.py`** â€” Use `JsonOutputParser` to get structured JSON output from the model
- **`structureOutputParser.py`** â€” Use `StructuredOutputParser` with `ResponseSchema` to define expected output fields

```bash
# Chaining prompts without a parser
python outputParsers/strOutputParserDemo.py

# StrOutputParser with chains
python outputParsers/strOutputParser.py

# JsonOutputParser
python outputParsers/jsonOutputParser.py

# StructuredOutputParser with ResponseSchema
python outputParsers/structureOutputParser.py
```

---

### 8. Chains â€” Simple, Sequential, Parallel & Conditional

> **Files:** `chains/`

Learn how to compose LangChain **chains** â€” from a basic single-step chain to advanced parallel and conditional routing:

| File                  | What It Does                                                                   |
| --------------------- | ------------------------------------------------------------------------------ |
| `simpleChain.py`      | Basic **prompt â†’ model â†’ parser** chain                                        |
| `sequentialChain.py`  | **Sequential chain** â€” generate a report then summarize it                     |
| `parallelChain.py`    | **RunnableParallel** â€” run notes & quiz branches concurrently, then merge      |
| `conditionalChain.py` | **RunnableBranch** â€” classify sentiment and route to positive/negative handler |

```bash
# Simple chain
python chains/simpleChain.py

# Sequential chain (report â†’ summary)
python chains/sequentialChain.py

# Parallel chain (notes + quiz â†’ merged doc)
python chains/parallelChain.py

# Conditional chain (sentiment-based routing)
python chains/conditionalChain.py
```

---

### 9. Runnables â€” Why LangChain Introduced the Runnable Interface

> **Files:** `runnables/`

Understand the **motivation behind Runnables** â€” the core abstraction in modern LangChain. These files walk through the problem the LangChain team faced and how they solved it:

| File                   | What It Does                                                                               |
| ---------------------- | ------------------------------------------------------------------------------------------ |
| `simpleLLM.py`         | The **old-way** LLM app â€” uses `.predict()` & manual `.format()` (no unified interface)    |
| `chainsProblem.ipynb`  | **The problem** â€” `fakeLLM`, `fakePromptTemplate`, `fakeLLMChain` each have different APIs |
| `chainsSolution.ipynb` | **The solution** â€” a `Runnable` ABC with a unified `.invoke()` & `runnableConnector`       |

**The Core Idea:**

- **Problem** (`chainsProblem.ipynb`): In early LangChain, every component had a different method â€” LLMs used `.predict()`, prompts used `.format()`, chains used `.run()`. This made chaining components inconsistent and hard to compose.
- **Solution** (`chainsSolution.ipynb`): Define a `Runnable` abstract base class where **every** component implements `.invoke()`. Then a `runnableConnector` can pipe output from one Runnable into the next â€” enabling the modern `prompt | model | parser` chain syntax.

```bash
# Old-way LLM usage
python runnables/simpleLLM.py

# Open the notebooks to explore the problem & solution
# chainsProblem.ipynb  â€” inconsistent interfaces
# chainsSolution.ipynb â€” Runnable with unified .invoke()
```

---

## ğŸ”‘ API Keys

| Provider    | Environment Variable       | Get Your Key                                                             |
| ----------- | -------------------------- | ------------------------------------------------------------------------ |
| OpenAI      | `OPENAI_API_KEY`           | [platform.openai.com](https://platform.openai.com/api-keys)              |
| HuggingFace | `HUGGINGFACEHUB_API_TOKEN` | [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) |

---

## ğŸ› ï¸ Tech Stack

- **[LangChain](https://www.langchain.com/)** â€” LLM application framework
- **[OpenAI](https://openai.com/)** â€” GPT-3.5 / GPT-4 models & embeddings
- **[HuggingFace](https://huggingface.co/)** â€” Open-source models (cloud & local)
- **[Pydantic](https://docs.pydantic.dev/)** â€” Data validation & structured output schemas
- **[Streamlit](https://streamlit.io/)** â€” Rapid web UI prototyping
- **[scikit-learn](https://scikit-learn.org/)** â€” Cosine similarity calculations
- **[python-dotenv](https://pypi.org/project/python-dotenv/)** â€” Environment variable management

---

## ğŸ“ License

This project is open source and available for learning purposes.

---

> **â­ If this repo helped you learn something new, consider giving it a star!**
